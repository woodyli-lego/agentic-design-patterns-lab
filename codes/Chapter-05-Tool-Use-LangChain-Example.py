import asyncio
import nest_asyncio

from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool as langchain_tool
from langchain.agents import create_agent


# ä½¿ç”¨æœ¬åœ° ollama
try:
   llm: ChatOllama = ChatOllama(model="qwen3:4b-instruct", temperature=0)
   print(f"Language model initialized: {llm.model}")
except Exception as e:
   print(f"Error initializing language model: {e}")
   raise RuntimeError("Failed to initialize language model. Please ensure Ollama is running and the model is available.") from e


# --- å®šä¹‰æ¨¡æ‹Ÿçš„æœç´¢å·¥å…· ---
@langchain_tool
def search_information(query: str) -> str:
   """
   Provides factual information on a given topic. Use this tool to find answers to phrases
   like 'capital of France' or 'weather in London?'.
   """
   print(f"\n--- ğŸ› ï¸ Tool Called: search_information with query: '{query}' ---")

   # é€šè¿‡ä¸€ä¸ªå­—å…¸é¢„å®šä¹‰çš„ç»“æœæ¥æ¨¡æ‹Ÿæœç´¢å·¥å…·ã€‚
   simulated_results = {
      "weather in london": "The weather in London is currently cloudy with a temperature of 15Â°C.",
      "capital of france": "The capital of France is Paris.",
      "population of earth": "The estimated population of Earth is around 8 billion people.",
      "tallest mountain": "Mount Everest is the tallest mountain above sea level.",
      "default": f"Simulated search result for '{query}': No specific information found, but the topic seems interesting."
   }
   result = simulated_results.get(query.lower(), simulated_results["default"])
   print(f"--- TOOL RESULT: {result} ---")
   return result

tools = [search_information]

# --- åˆ›å»ºä¸€ä¸ªä½¿ç”¨å·¥å…·çš„æ™ºèƒ½ä½“ ---
# è¿™ä¸ªæç¤ºæ¨¡æ¿éœ€è¦ä¸€ä¸ª `agent_scratchpad` å ä½ç¬¦ï¼Œç”¨äºè®°å½•æ™ºèƒ½ä½“çš„å†…éƒ¨æ­¥éª¤ã€‚
agent_prompt = ChatPromptTemplate.from_messages([
   ("system", "You are a helpful assistant."),
   ("human", "{input}"),
   ("placeholder", "{agent_scratchpad}"),
])

# ä½¿ç”¨å®šä¹‰å¥½çš„å¤§è¯­è¨€æ¨¡å‹ã€å·¥å…·å’Œæç¤ºè¯æ¨¡æ¿æ„å»ºæ™ºèƒ½ä½“ã€‚
agent = create_agent(model=llm, tools=tools, system_prompt="You are a helpful assistant")

async def run_agent_with_tool(query: str):
   """
   Invokes the agent executor with a query and prints the final response.
   """
   print(f"\n--- ğŸƒ Running Agent with Query: '{query}' ---")
   try:
      response = await agent.ainvoke({"messages": [{"role": "user", "content": query}]})

      print("\n--- âœ… Final Agent Response ---")
      print(response["messages"][ -1 ].content)
   except Exception as e:
      print(f"\nğŸ›‘ An error occurred during agent execution: {e}")

async def main():
   """
   Runs all agent queries concurrently.
   """
   tasks = [
      run_agent_with_tool("What is the capital of France?"),
      run_agent_with_tool("What's the weather like in London?"),
      # ä¸çŸ¥é“æ˜¯æ€ä¹ˆåšåˆ°çš„ï¼Œä½†æ˜¯ tool çš„æ–‡æ¡£åº”è¯¥æ˜ç¡®è¯´æ˜æ”¯æŒ `population of earth` å…³é”®å­—ã€‚
      run_agent_with_tool("Tell me the population of Earth."),
      run_agent_with_tool("Tell me something about dogs.") # Should trigger the default tool response
   ]
   await asyncio.gather(*tasks)

nest_asyncio.apply()
asyncio.run(main())
